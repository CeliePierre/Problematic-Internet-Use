{"metadata":{"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":81933,"databundleVersionId":9643020,"sourceType":"competition"}],"dockerImageVersionId":30786,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false},"papermill":{"default_parameters":{},"duration":35.096282,"end_time":"2024-12-02T05:26:22.781316","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-12-02T05:25:47.685034","version":"2.6.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Loading The Data Sets\n","metadata":{"papermill":{"duration":0.003992,"end_time":"2024-12-02T05:25:50.795310","exception":false,"start_time":"2024-12-02T05:25:50.791318","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Import libraries\nimport numpy as np\nimport pandas as pd\nimport os\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.impute import SimpleImputer\nfrom concurrent.futures import ThreadPoolExecutor\nfrom tqdm import tqdm\nimport missingno as msno","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-12-02T05:25:50.804049Z","iopub.status.busy":"2024-12-02T05:25:50.803381Z","iopub.status.idle":"2024-12-02T05:25:54.646694Z","shell.execute_reply":"2024-12-02T05:25:54.645209Z"},"papermill":{"duration":3.850311,"end_time":"2024-12-02T05:25:54.649084","exception":false,"start_time":"2024-12-02T05:25:50.798773","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cite: https://www.kaggle.com/code/cchangyyy/0-490-notebook\n\n\n# Processes a parquet file\ndef process_file(filename, dirname):\n    df = pd.read_parquet(os.path.join(dirname, filename, \"part-0.parquet\"))\n    df.drop(\"step\", axis=1, inplace=True)\n    return df.describe().values.reshape(-1), filename.split(\"=\")[1]\n\n\ndef load_time_series(dirname) -> pd.DataFrame:\n    ids = os.listdir(dirname)\n    with ThreadPoolExecutor() as executor:\n        results = list(\n            tqdm(\n                executor.map(lambda fname: process_file(fname, dirname), ids),\n                total=len(ids),\n            )\n        )\n    stats, indexes = zip(*results)\n    df = pd.DataFrame(stats, columns=[f\"stat_{i}\" for i in range(len(stats[0]))])\n    df[\"id\"] = indexes\n    return df","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Loading the train and test data sets\n# Aligning train dataset columns to match test dataset\n# Checking for column differences between train and test datasets\n\n# CP: Running locally\nif os.path.exists(\"kaggle_data\"):\n    train_data = pd.read_csv(\"kaggle_data/train.csv\")\n    test_data = pd.read_csv(\"kaggle_data/test.csv\")\n    train_ts = load_time_series(\"kaggle_data/series_train.parquet\")\n    test_ts = load_time_series(\"kaggle_data/series_test.parquet\")\n\n# CP: Running in Kaggle\nelse:\n    train_data = pd.read_csv(\n        \"/kaggle/input/child-mind-institute-problematic-internet-use/train.csv\"\n    )\n    test_data = pd.read_csv(\n        \"/kaggle/input/child-mind-institute-problematic-internet-use/test.csv\"\n    )\n    train_ts = load_time_series(\n        \"/kaggle/input/child-mind-institute-problematic-internet-use/series_train.parquet\"\n    )\n    test_ts = load_time_series(\n        \"/kaggle/input/child-mind-institute-problematic-internet-use/series_test.parquet\"\n    )\n\n\ncolumn_names = list(test_data.columns)\ntarget = train_data[\"sii\"]\ntrain_data = pd.DataFrame(train_data, columns=column_names)\ntrain_data[\"sii\"] = target\nprint(train_data.columns.difference(test_data.columns))\nprint(train_data.shape)\nprint(test_data.shape)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cite: https://www.kaggle.com/code/cchangyyy/0-490-notebook\n\ntime_series_cols = train_ts.columns.tolist()\ntime_series_cols.remove(\"id\")\ntrain_data = pd.merge(train_data, train_ts, how=\"left\", on=\"id\")\ntest_data = pd.merge(test_data, test_ts, how=\"left\", on=\"id\")\ntrain_data = train_data.drop(\"id\", axis=1)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Make a copy (comment on/off)\n# train_data_copy = train_data.copy()\n# test_data_copy = test_data.copy()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Display all rows\npd.set_option(\"display.max_rows\", None)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Data Visualization\n","metadata":{}},{"cell_type":"code","source":"plt.hist(train_data[\"sii\"].dropna(), bins=4, edgecolor=\"k\")\nplt.title(\"Distribution of SII\")\nplt.xlabel(\"SII\")\nplt.ylabel(\"Frequency\")\nplt.show()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Missing SII data\n\nsii_val = train_data[\"sii\"].value_counts().sum()\nsii_missing = train_data[\"sii\"].isnull().sum()\ntotal_rows = train_data.shape[0]\n\nprint(f\"Total number of SII values: {sii_val}\")\nprint(f\"Total number of missing SII values: {sii_missing}\")\nprint(f\"Percentage of missing SII values: {sii_missing/total_rows*100:.2f}%\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Missing csv data\nmissing_csv = (\n    train_data.loc[:, ~train_data.columns.str.startswith(\"stat_\")].isnull().sum().sum()\n)\nprint(\"Total missing csv data: \", missing_csv)\nprint(\"Percentage of missing csv data: \", missing_csv / train_data.size * 100)\ntrain_data.loc[\n    :, ~train_data.columns.str.startswith(\"stat_\")\n].isnull().sum().sort_values(ascending=False)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Missing parquet data\nmissing_parquet = (\n    train_data.loc[:, train_data.columns.str.startswith(\"stat_\")].isnull().sum().sum()\n)\nprint(\"Total missing parquet data: \", missing_parquet)\nprint(\"Percentage of missing parquet data: \", missing_parquet / train_data.size * 100)\ntrain_data.loc[\n    :, train_data.columns.str.startswith(\"stat_\")\n].isnull().sum().sort_values(ascending=False)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_cat = train_data.select_dtypes(include=[\"object\"])\ndf_num = train_data.select_dtypes(include=[\"int64\", \"float64\"])","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Data visualization for numerical data\nfor i in df_num.columns:\n    unique_values = df_num[i].dropna().unique()\n    # Check for binary data\n    if len(unique_values) == 2:\n        sns.countplot(x=df_num[i])\n        plt.title(f\"Binary Distribution of {i}\")\n    else:\n        plt.hist(df_num[i])\n        plt.title(f\"Distribution of {i}\")\n    plt.show()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Data visualization for categorical data\nfor i in df_cat.columns:\n    sns.barplot(x=df_cat[i].value_counts().index, y=df_cat[i].value_counts()).set_title(\n        i\n    )\n    plt.show()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Data Preprocessing\n","metadata":{"papermill":{"duration":0.003381,"end_time":"2024-12-02T05:25:54.655910","exception":false,"start_time":"2024-12-02T05:25:54.652529","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Dropping the 'id' column from both train and test datasets\nids = test_data[\"id\"]\ntest_data = test_data.drop(\"id\", axis=1)\n\n# Apply one-hot encoding to categorical columns\ntrain_data = pd.get_dummies(train_data)\ntest_data = pd.get_dummies(test_data)\n\n# Align train and test datasets to ensure they have the same columns\n# Missing columns will be added with NaN values\ntrain_data, test_data = train_data.align(test_data, join=\"outer\", axis=1)\n\n# Fill missing values with 0 (useful for alignment step)\ntrain_data.fillna(value=0, inplace=True)\ntest_data.fillna(value=0, inplace=True)\n\n# Print the shapes of the datasets to verify alignment\nprint(f\"Train data shape: {train_data.shape}\")\nprint(f\"Test data shape: {test_data.shape}\")\n\n# Display dataset info to inspect column data types and memory usage\nprint(\"\\nTrain data info:\")\nprint(train_data.info())\nprint(\"\\nTest data info:\")\nprint(test_data.info())\n\n# Check for any column differences\ndifference = train_data.columns.difference(test_data.columns)\nif difference.empty:\n    print(\"No column differences between train and test datasets.\")\nelse:\n    print(\"Column differences found:\", difference)\n\n# Remove target column 'sii' from test dataset, as it's not available for predictions\ntest_data = test_data.drop(columns=[\"sii\"], errors=\"ignore\")\n\n# Display final column details\nprint(\"\\nFinal train columns:\", train_data.columns)\nprint(\"\\nFinal test columns:\", test_data.columns)","metadata":{"execution":{"iopub.execute_input":"2024-12-02T05:25:54.664733Z","iopub.status.busy":"2024-12-02T05:25:54.664291Z","iopub.status.idle":"2024-12-02T05:25:54.745601Z","shell.execute_reply":"2024-12-02T05:25:54.744226Z"},"papermill":{"duration":0.089378,"end_time":"2024-12-02T05:25:54.748791","exception":false,"start_time":"2024-12-02T05:25:54.659413","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Random Forest Model Predictions\n","metadata":{"papermill":{"duration":0.003368,"end_time":"2024-12-02T05:25:54.756009","exception":false,"start_time":"2024-12-02T05:25:54.752641","status":"completed"},"tags":[]}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom imblearn.over_sampling import SMOTE\nimport numpy as np\nimport pandas as pd\n\n# Prepare features and target\nX = train_data.drop(columns=[\"sii\"])\ny = train_data[\"sii\"]\n\n# Scale features\nscaler = MinMaxScaler()\nX = scaler.fit_transform(X)\n\n# Apply SMOTE for class imbalance\nsmote = SMOTE(random_state=42)\nX_resampled, y_resampled = smote.fit_resample(X, y)\n\n# Split data into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(\n    X_resampled, y_resampled, test_size=0.1, random_state=42\n)\n\n# Initialize Random Forest with adjusted parameters\nRFC = RandomForestClassifier(\n    n_estimators=150,  # Reduced number of trees for faster evaluation\n    max_depth=10,  # Lower depth for simpler trees\n    min_samples_split=10,  # Increase minimum samples required to split\n    min_samples_leaf=5,  # Require more samples in leaf nodes\n    class_weight=\"balanced\",  # Emphasize minority classes\n    random_state=42,\n)\n\n# Fit the model\nRFC.fit(X_train, y_train)\n\n# Predict on train and test data\ny_pred_test = RFC.predict(X_test)\ny_pred_train = RFC.predict(X_train)\n\n# Evaluate model\nprint(\n    \"Testing data: Model accuracy score : {0:0.4f}\".format(\n        accuracy_score(y_test, y_pred_test) * 100\n    )\n)\nprint(\n    \"Training data: Model accuracy score : {0:0.4f}\".format(\n        accuracy_score(y_train, y_pred_train) * 100\n    )\n)\n\n# Classification report for detailed metrics\nprint(\"\\nClassification Report for Testing Data:\")\nprint(classification_report(y_test, y_pred_test))\n\n# Cross-validation scores for model stability\ncv_scores = cross_val_score(RFC, X_resampled, y_resampled, cv=5, scoring=\"accuracy\")\nprint(\"\\nCross-validation Accuracy Scores:\", cv_scores)\nprint(\"Mean CV Accuracy: {0:0.4f}\".format(np.mean(cv_scores)))\n\n# Feature Importance\nfeature_importances = pd.DataFrame(\n    {\n        \"Feature\": train_data.drop(columns=[\"sii\"]).columns,\n        \"Importance\": RFC.feature_importances_,\n    }\n).sort_values(by=\"Importance\", ascending=False)\n\nprint(\"\\nTop Features by Importance:\")\nprint(feature_importances.head(10))","metadata":{"execution":{"iopub.execute_input":"2024-12-02T05:25:54.765787Z","iopub.status.busy":"2024-12-02T05:25:54.765381Z","iopub.status.idle":"2024-12-02T05:26:21.506116Z","shell.execute_reply":"2024-12-02T05:26:21.504877Z"},"papermill":{"duration":26.748825,"end_time":"2024-12-02T05:26:21.508660","exception":false,"start_time":"2024-12-02T05:25:54.759835","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Get feature importances\ncolumns = train_data.drop(columns=[\"sii\"]).columns\n\nimportances = RFC.feature_importances_\nindices = np.argsort(importances)[::-1]\n\n# # Summarize feature importances\n# print(\"Feature ranking:\")\n# for f in range(X_train.shape[1]):\n#     print(f\"{f + 1}. feature {columns[indices[f]]} ({importances[indices[f]]:.3f})\")\n\n# Plot the feature importances\nplt.figure()\nplt.title(\"Feature importances\")\nplt.bar(range(X_train.shape[1]), importances[indices], color=\"r\", align=\"center\")\nplt.xticks(range(X_train.shape[1]), [columns[i] for i in indices], rotation=90)\nplt.xlim([-1, X_train.shape[1]])\nplt.show()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plot ROC curve\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import roc_curve, roc_auc_score\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.preprocessing import label_binarize\n\n# Binarize the output\ny_train_bin = label_binarize(y_train, classes=[0, 1, 2, 3])\n\n# Use OneVsRestClassifier for multiclass ROC curve\nclassifier = OneVsRestClassifier(RFC)\ny_pred_train = classifier.fit(X_train, y_train_bin).predict_proba(X_train)\n\n# Compute ROC curve and ROC area for each class\nfpr = dict()\ntpr = dict()\nroc_auc = dict()\nfor i in range(y_train_bin.shape[1]):\n    fpr[i], tpr[i], _ = roc_curve(y_train_bin[:, i], y_pred_train[:, i])\n    roc_auc[i] = roc_auc_score(y_train_bin[:, i], y_pred_train[:, i])\n\n# Plot ROC curve for each class\nplt.figure(figsize=(8, 6))\ncolors = [\"aqua\", \"darkorange\", \"cornflowerblue\", \"green\"]\nfor i, color in zip(range(y_train_bin.shape[1]), colors):\n    plt.plot(\n        fpr[i],\n        tpr[i],\n        color=color,\n        lw=2,\n        label=f\"ROC curve of class {i} (area = {roc_auc[i]:.2f})\",\n    )\n\nplt.plot([0, 1], [0, 1], color=\"navy\", lw=2, linestyle=\"--\")\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"Receiver Operating Characteristic\")\nplt.legend(loc=\"lower right\")\nplt.show()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn import metrics\n\n# Plot confusion matrix\ny_pred = RFC.predict(X_test)\nconf_matrix = metrics.confusion_matrix(y_test, y_pred)\n\nplt.figure(figsize=(10, 7))\nsns.heatmap(\n    conf_matrix,\n    annot=True,\n    fmt=\"d\",\n    cmap=\"Blues\",\n    xticklabels=[0, 1, 2, 3],\n    yticklabels=[0, 1, 2, 3],\n)\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")\nplt.title(\"Confusion Matrix\")\nplt.show()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Random Forest Predictions On Test Set\n","metadata":{"papermill":{"duration":0.004004,"end_time":"2024-12-02T05:26:21.968878","exception":false,"start_time":"2024-12-02T05:26:21.964874","status":"completed"},"tags":[]}},{"cell_type":"code","source":"X = test_data\nX = scaler.fit_transform(X)\ny_pred = RFC.predict(X)\n\nsubmission = pd.DataFrame({\"id\": ids, \"sii\": y_pred.astype(int)})\nprint(submission)\n\nsubmission.to_csv(\"submission.csv\", index=False)\nprint(\"Submission file created.\")","metadata":{"execution":{"iopub.execute_input":"2024-12-02T05:26:21.979291Z","iopub.status.busy":"2024-12-02T05:26:21.978925Z","iopub.status.idle":"2024-12-02T05:26:22.009391Z","shell.execute_reply":"2024-12-02T05:26:22.007953Z"},"papermill":{"duration":0.038694,"end_time":"2024-12-02T05:26:22.011867","exception":false,"start_time":"2024-12-02T05:26:21.973173","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Logistic Regression Model Predictions\n","metadata":{"papermill":{"duration":0.003465,"end_time":"2024-12-02T05:26:21.516179","exception":false,"start_time":"2024-12-02T05:26:21.512714","status":"completed"},"tags":[]}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\nX = train_data.drop(columns=[\"sii\"])\ny = train_data[\"sii\"]\n\nscaler = MinMaxScaler()\nX = scaler.fit_transform(X)\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.1, random_state=42\n)\n\nLR = LogisticRegression()\nLR.fit(X_train, y_train)\n\ny_pred_test = LR.predict(X_test)\ny_pred_train = RFC.predict(X_train)\n\nprint(\n    \"Testing data accuracy: {0:0.4f}\".format(accuracy_score(y_test, y_pred_test) * 100)\n)\nprint(\n    \"Training data accuracy: {0:0.4f}\".format(\n        accuracy_score(y_train, y_pred_train) * 100\n    )\n)","metadata":{"execution":{"iopub.execute_input":"2024-12-02T05:26:21.525561Z","iopub.status.busy":"2024-12-02T05:26:21.525000Z","iopub.status.idle":"2024-12-02T05:26:21.958030Z","shell.execute_reply":"2024-12-02T05:26:21.956573Z"},"papermill":{"duration":0.440776,"end_time":"2024-12-02T05:26:21.960711","exception":false,"start_time":"2024-12-02T05:26:21.519935","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Logistic Regression Predictions On Test Set\n","metadata":{"papermill":{"duration":0.004748,"end_time":"2024-12-02T05:26:22.020826","exception":false,"start_time":"2024-12-02T05:26:22.016078","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Predicting on test data\n# X = test_data\n# X = scaler.fit_transform(X)\n# y_pred = LR.predict(X)\n\n# submission = pd.DataFrame({\n#     'id': ids,\n#     'sii': y_pred.astype(int)\n# })\n\n# #saving to CSV\n# submission.to_csv('submission.csv', index=False)\n# print(submission)","metadata":{"execution":{"iopub.execute_input":"2024-12-02T05:26:22.030634Z","iopub.status.busy":"2024-12-02T05:26:22.030203Z","iopub.status.idle":"2024-12-02T05:26:22.035179Z","shell.execute_reply":"2024-12-02T05:26:22.034127Z"},"papermill":{"duration":0.012814,"end_time":"2024-12-02T05:26:22.037720","exception":false,"start_time":"2024-12-02T05:26:22.024906","status":"completed"},"tags":[]},"outputs":[],"execution_count":null}]}