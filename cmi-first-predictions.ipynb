{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":81933,"databundleVersionId":9643020,"sourceType":"competition"}],"dockerImageVersionId":30786,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false},"papermill":{"default_parameters":{},"duration":8.795735,"end_time":"2024-11-28T22:01:29.420849","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-11-28T22:01:20.625114","version":"2.6.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Loading The Data Sets\n","metadata":{"papermill":{"duration":0.003807,"end_time":"2024-11-28T22:01:23.512150","exception":false,"start_time":"2024-11-28T22:01:23.508343","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Load the libraries\n\nimport numpy as np\n\nimport pandas as pd\n\nimport os\n\nimport seaborn as sns\n\nimport matplotlib.pyplot as plt\n\nfrom pandas.plotting import andrews_curves\n\nfrom sklearn.impute import SimpleImputer\n\nfrom concurrent.futures import ThreadPoolExecutor\n\nfrom tqdm import tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T00:33:58.878046Z","iopub.execute_input":"2024-12-02T00:33:58.878382Z","iopub.status.idle":"2024-12-02T00:33:58.884789Z","shell.execute_reply.started":"2024-12-02T00:33:58.878350Z","shell.execute_reply":"2024-12-02T00:33:58.883710Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# CP: Processes a parquet file\n\n# Cite: https://www.kaggle.com/code/cchangyyy/0-490-notebook\n\n\n\n\n\ndef process_file(filename, dirname):\n\n    df = pd.read_parquet(os.path.join(dirname, filename, \"part-0.parquet\"))\n\n    df.drop(\"step\", axis=1, inplace=True)\n\n    return df.describe().values.reshape(-1), filename.split(\"=\")[1]\n\n\n\n\n\ndef load_time_series(dirname) -> pd.DataFrame:\n\n    ids = os.listdir(dirname)\n\n    with ThreadPoolExecutor() as executor:\n\n        results = list(\n\n            tqdm(\n\n                executor.map(lambda fname: process_file(fname, dirname), ids),\n\n                total=len(ids),\n\n            )\n\n        )\n\n    stats, indexes = zip(*results)\n\n    df = pd.DataFrame(stats, columns=[f\"stat_{i}\" for i in range(len(stats[0]))])\n\n    df[\"id\"] = indexes\n\n    return df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T00:33:58.886315Z","iopub.execute_input":"2024-12-02T00:33:58.886803Z","iopub.status.idle":"2024-12-02T00:33:58.899880Z","shell.execute_reply.started":"2024-12-02T00:33:58.886751Z","shell.execute_reply":"2024-12-02T00:33:58.898839Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# CP: Load data\n\n# CP: Check if you are running in Kaggle or locally\n\n\n\n# CP: Running locally\n\nif os.path.exists(\"kaggle_data\"):\n\n    train_data = pd.read_csv(\"kaggle_data/train.csv\")\n\n    test_data = pd.read_csv(\"kaggle_data/test.csv\")\n\n    data_dict = pd.read_csv(\"kaggle_data/data_dictionary.csv\")\n\n    train_ts = load_time_series(\"kaggle_data/series_train.parquet\")\n\n    test_ts = load_time_series(\"kaggle_data/series_test.parquet\")\n\n\n\n# CP: Running in Kaggle\n\nelse:\n\n    train_data = pd.read_csv(\n\n        \"/kaggle/input/child-mind-institute-problematic-internet-use/train.csv\"\n\n    )\n\n    test_data = pd.read_csv(\n\n        \"/kaggle/input/child-mind-institute-problematic-internet-use/test.csv\"\n\n    )\n\n    data_dict = pd.read_csv(\n\n        \"/kaggle/input/child-mind-institute-problematic-internet-use/data_dictionary.csv\"\n\n    )\n\n    train_ts = load_time_series(\n\n        \"/kaggle/input/child-mind-institute-problematic-internet-use/series_train.parquet\"\n\n    )\n\n    test_ts = load_time_series(\n\n        \"/kaggle/input/child-mind-institute-problematic-internet-use/series_test.parquet\"\n\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T00:33:58.903296Z","iopub.execute_input":"2024-12-02T00:33:58.903750Z","iopub.status.idle":"2024-12-02T00:35:20.499942Z","shell.execute_reply.started":"2024-12-02T00:33:58.903711Z","shell.execute_reply":"2024-12-02T00:35:20.498823Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cite: https://www.kaggle.com/code/cchangyyy/0-490-notebook\n\n\n\ntime_series_cols = train_ts.columns.tolist()\n\ntime_series_cols.remove(\"id\")\n\ntrain_data = pd.merge(train_data, train_ts, how=\"left\", on=\"id\")\n\ntest_data = pd.merge(test_data, test_ts, how=\"left\", on=\"id\")\n\ntrain_data = train_data.drop(\"id\", axis=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T00:35:20.501325Z","iopub.execute_input":"2024-12-02T00:35:20.501732Z","iopub.status.idle":"2024-12-02T00:35:20.522229Z","shell.execute_reply.started":"2024-12-02T00:35:20.501694Z","shell.execute_reply":"2024-12-02T00:35:20.521165Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"column_names = list(test_data.columns)\n\n\n\ntarget = train_data[\"sii\"]\n\ntrain_data = pd.DataFrame(train_data, columns=column_names)\n\n\n\ntrain_data[\"sii\"] = target\n\n\n\nprint(train_data.columns.difference(test_data.columns))\n\nprint(train_data.shape)\n\nprint(test_data.shape)","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2024-12-02T00:35:20.523513Z","iopub.execute_input":"2024-12-02T00:35:20.523886Z","iopub.status.idle":"2024-12-02T00:35:20.535190Z","shell.execute_reply.started":"2024-12-02T00:35:20.523841Z","shell.execute_reply":"2024-12-02T00:35:20.533937Z"},"papermill":{"duration":3.394442,"end_time":"2024-11-28T22:01:26.910156","exception":false,"start_time":"2024-11-28T22:01:23.515714","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Data Preprocessing\n","metadata":{"papermill":{"duration":0.003105,"end_time":"2024-11-28T22:01:26.916751","exception":false,"start_time":"2024-11-28T22:01:26.913646","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Dropping ID columns.\n\n# Saving ID column for Kaggle prediction.\n\nids = test_data[\"id\"]\n\n\n\ntrain_data = train_data.drop(\"id\", axis=1)\n\ntest_data = test_data.drop(\"id\", axis=1)\n\n\n\n# Using one hot encoding on the categorical data.\n\n# Cite: https://stackoverflow.com/questions/41973423/typeerror-dataframe-object-is-not-callable\n\n# Cite: https://www.kaggle.com/code/dansbecker/using-categorical-data-with-one-hot-encoding\n\ntrain_data = pd.get_dummies(train_data)\n\ntest_data = pd.get_dummies(test_data)\n\ntrain_data, test_data = train_data.align(test_data, join=\"outer\", axis=1)\n\n\n\n# mean = train_data.mean()\n\n# Cite: https://www.kaggle.com/code/dansbecker/handling-missing-values\n\ntrain_data.fillna(value=0, inplace=True)\n\ntest_data.fillna(value=0, inplace=True)\n\n\n\n# Imputing missing data with SimpleImputer\n\n# imputer = SimpleImputer()\n\n# imputed_data = imputer.fit_transform(train_data)\n\n# train_data = pd.DataFrame(imputed_data, columns=train_data.columns)\n\n\n\n\n\nprint(train_data.shape)\n\nprint(test_data.shape)\n\n\n\nprint(train_data.info())\n\nprint(test_data.info())\n\n\n\ndifference = train_data.columns.difference(test_data.columns)\n\nprint(difference)\n\n\n\ntest_data = test_data.drop(columns=[\"sii\"])","metadata":{"execution":{"iopub.status.busy":"2024-12-02T00:35:20.536543Z","iopub.execute_input":"2024-12-02T00:35:20.536916Z","iopub.status.idle":"2024-12-02T00:35:20.604061Z","shell.execute_reply.started":"2024-12-02T00:35:20.536880Z","shell.execute_reply":"2024-12-02T00:35:20.602722Z"},"papermill":{"duration":0.082098,"end_time":"2024-11-28T22:01:27.002688","exception":false,"start_time":"2024-11-28T22:01:26.920590","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Random Forest Model Predictions\n","metadata":{"papermill":{"duration":0.003356,"end_time":"2024-11-28T22:01:27.010406","exception":false,"start_time":"2024-11-28T22:01:27.007050","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Cite: https://www.kaggle.com/code/prashant111/random-forest-classifier-tutorial\n\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.preprocessing import MinMaxScaler\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nfrom sklearn.metrics import accuracy_score, classification_report\n\n\n\nX = train_data.drop(columns=[\"sii\"])\n\ny = train_data[\"sii\"]\n\n\n\n# Scaling the training data.\n\nscaler = MinMaxScaler()\n\nX = scaler.fit_transform(X)\n\n\n\n# Splitting the training and testing data\n\nX_train, X_test, y_train, y_test = train_test_split(\n\n    X, y, test_size=0.1, random_state=42\n\n)\n\n\n\n# Initiating the random forrest model\n\nRFC = RandomForestClassifier(n_estimators=100, random_state=42)\n\n\n\n# Fitting the model\n\nRFC.fit(X_train, y_train)\n\n\n\n\n\n# Predicting test set results\n\ny_pred_test = RFC.predict(X_test)\n\ny_pred_train = RFC.predict(X_train)\n\n\n\n# Checking accuracy score\n\nprint(\n\n    \"Testing data: Model accuracy score with 100 decision-trees : {0:0.4f}\".format(\n\n        accuracy_score(y_test, y_pred_test) * 100\n\n    )\n\n)\n\nprint(\n\n    \"Training data: Model accuracy score with 100 decision-trees : {0:0.4f}\".format(\n\n        accuracy_score(y_train, y_pred_train) * 100\n\n    )\n\n)","metadata":{"execution":{"iopub.status.busy":"2024-12-02T00:35:20.605706Z","iopub.execute_input":"2024-12-02T00:35:20.606744Z","iopub.status.idle":"2024-12-02T00:35:21.987442Z","shell.execute_reply.started":"2024-12-02T00:35:20.606686Z","shell.execute_reply":"2024-12-02T00:35:21.986272Z"},"papermill":{"duration":1.197939,"end_time":"2024-11-28T22:01:28.211913","exception":false,"start_time":"2024-11-28T22:01:27.013974","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Logistic Regression Model Predictions\n","metadata":{"papermill":{"duration":0.003386,"end_time":"2024-11-28T22:01:28.219309","exception":false,"start_time":"2024-11-28T22:01:28.215923","status":"completed"},"tags":[]}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\n\n\nX = train_data.drop(columns=[\"sii\"])\n\ny = train_data[\"sii\"]\n\n\n\nscaler = MinMaxScaler()\n\nX = scaler.fit_transform(X)\n\n\n\nX_train, X_test, y_train, y_test = train_test_split(\n\n    X, y, test_size=0.1, random_state=42\n\n)\n\n\n\nLR = LogisticRegression()\n\n\n\nLR.fit(X_train, y_train)\n\n\n\ny_pred_test = LR.predict(X_test)\n\ny_pred_train = RFC.predict(X_train)\n\n\n\nprint(\n\n    \"Testing data accuracy: {0:0.4f}\".format(accuracy_score(y_test, y_pred_test) * 100)\n\n)\n\nprint(\n\n    \"Training data accuracy: {0:0.4f}\".format(\n\n        accuracy_score(y_train, y_pred_train) * 100\n\n    )\n\n)","metadata":{"execution":{"iopub.status.busy":"2024-12-02T00:35:47.372960Z","iopub.execute_input":"2024-12-02T00:35:47.373373Z","iopub.status.idle":"2024-12-02T00:35:47.781935Z","shell.execute_reply.started":"2024-12-02T00:35:47.373337Z","shell.execute_reply":"2024-12-02T00:35:47.778694Z"},"papermill":{"duration":0.402475,"end_time":"2024-11-28T22:01:28.625532","exception":false,"start_time":"2024-11-28T22:01:28.223057","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Random Forest Predictions On Test Set\n","metadata":{"papermill":{"duration":0.003823,"end_time":"2024-11-28T22:01:28.633308","exception":false,"start_time":"2024-11-28T22:01:28.629485","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# X = test_data\n\n# X = scaler.fit_transform(X)\n\n# y_pred = RFC.predict(X)\n\n# Predict on test data\n\n\n\n# Creating submission file\n\n# submission = pd.DataFrame({\n\n#   'id': ids,\n\n#   'sii': y_pred.astype(int)\n\n# })\n\n# print(submission)\n\n\n\n# save to CSV\n\n# submission.to_csv('submission.csv', index=False)\n\n# print(\"Submission file created.\")","metadata":{"execution":{"iopub.status.busy":"2024-12-02T00:35:53.534269Z","iopub.execute_input":"2024-12-02T00:35:53.534728Z","iopub.status.idle":"2024-12-02T00:35:53.540016Z","shell.execute_reply.started":"2024-12-02T00:35:53.534688Z","shell.execute_reply":"2024-12-02T00:35:53.538797Z"},"papermill":{"duration":0.011511,"end_time":"2024-11-28T22:01:28.648607","exception":false,"start_time":"2024-11-28T22:01:28.637096","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Logistic Regression Predictions On Test Set\n","metadata":{"papermill":{"duration":0.004442,"end_time":"2024-11-28T22:01:28.656849","exception":false,"start_time":"2024-11-28T22:01:28.652407","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Predicting on test data\n\nX = test_data\n\nX = scaler.fit_transform(X)\n\ny_pred = LR.predict(X)\n\n\n\n# Creating the submission file\n\nsubmission = pd.DataFrame({\"id\": ids, \"sii\": y_pred.astype(int)})\n\n\n\n# saving to CSV\n\nsubmission.to_csv(\"submission.csv\", index=False)\n\nprint(submission)","metadata":{"execution":{"iopub.status.busy":"2024-12-02T00:35:22.407109Z","iopub.execute_input":"2024-12-02T00:35:22.407641Z","iopub.status.idle":"2024-12-02T00:35:22.433812Z","shell.execute_reply.started":"2024-12-02T00:35:22.407564Z","shell.execute_reply":"2024-12-02T00:35:22.432523Z"},"papermill":{"duration":0.026177,"end_time":"2024-11-28T22:01:28.686675","exception":false,"start_time":"2024-11-28T22:01:28.660498","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"For both models, I split the training and testing data 90/10 with random_state set to 42. \n\nRandom Forrest Classifier Peramters: n_estimators=100, *, criterion='gini', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='sqrt', max_leaf_nodes=None, min_impurity_decrease=0.0, bootstrap=True, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, class_weight=None, ccp_alpha=0.0, max_samples=None, monotonic_cst=None\n\nLogistic Regression Parameters: penalty='l2', *, dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver='lbfgs', max_iter=100, multi_class='deprecated', verbose=0, warm_start=False, n_jobs=None, l1_ratio=None\n\nAfter I receive my results from Kaggle, I'm expecting to get around 74% accuracy on the RFC model, and 72% accuracy on the LR model. I got these estimates from the two model performance results on the train/test split data that I printed. \n\nThe accuracy scores I received from Kaggle on the test set they provided were 27% for the Logistic Regression model and 17% accuracy for the Random Forrest classifier model. When comparing these scores to the LR 72% and RFC 74% scores from my training set, it's clear that both models are overfitting the data, have low bias, and have high variance. \n\nThe performance of my models shows that they both overfit the data and need to be generalized more. I plan to improve this accuracy in a couple of areas. First I will change the way I handle missing data by instead of replacing missing data points with 0s, I will replace them with the mean of that column. I will also improve my data's regularization and make the models more generalized by removing features that don't correlate too well with the data. I also plan on tuning my parameters and experimenting with which ones affect model performance. Additionally, I may experiment with running cross-validation on the training data to help further prevent overfitting.   \n\nMy biggest takeaway from this assignment is that preprocessing is the most crucial and important part of creating an accurate machine-learning model. It doesn't matter how well-prepared my model or parameters are, if I don't have clean data, I won't get accurate predictions. I learned while doing this assignment that the more steps I take to preprocess and clean up my datasets, the more accurate my models are.","metadata":{}}]}